{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1314f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "NUM_DATA = 31231\n",
    "IMG_W = 48\n",
    "IMG_H = 48\n",
    "#CHANGE VALIDATION AND TRAINING LABELS AXIS\n",
    "#CHANGE DROP_REMAINDER FOR BATCHES\n",
    "\n",
    "trainPath  = r\"C:\\Users\\Aiden\\OneDrive\\Documents\\Hackathon\\Data\\trainBackup.csv\"\n",
    "testPath = r\"C:\\Users\\Aiden\\OneDrive\\Documents\\Hackathon\\Data\\test.csv\"\n",
    "dataPath = r\"C:\\Users\\Aiden\\OneDrive\\Documents\\Hackathon\\data\\icml_face_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9bef5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, json, shutil, pprint, csv, sys, re, pickle\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import IPython.display as display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "print(\"Tensorflow version \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c68f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.rc('image', cmap='gray_r')\n",
    "plt.rc('grid', linewidth=1)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0', figsize=(16,9))\n",
    "# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "def plot_learning_rate(lr_func, epochs):\n",
    "    xx = np.arange(epochs+1, dtype=np.float64)\n",
    "    y = [lr_decay(x) for x in xx]\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_title('Learning rate\\ndecays from {:0.3g} to {:0.3g}'.format(y[0], y[-2]))\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
    "    ax.grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
    "    ax.step(xx,y, linewidth=3, where='post')\n",
    "    display.display(fig)\n",
    "\n",
    "class PlotTraining(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, sample_rate=1, zoom=1):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.step = 0\n",
    "        self.zoom = zoom\n",
    "        self.steps_per_epoch = NUM_DATA//BATCH_SIZE\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.batch_history = {}\n",
    "        self.batch_step = []\n",
    "        self.epoch_history = {}\n",
    "        self.epoch_step = []\n",
    "        self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        plt.ioff()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (batch % self.sample_rate) == 0:\n",
    "            self.batch_step.append(self.step)\n",
    "            for k,v in logs.items():\n",
    "                # do not log \"batch\" and \"size\" metrics that do not change\n",
    "                # do not log training accuracy \"acc\"\n",
    "                if k=='batch' or k=='size':# or k=='acc':\n",
    "                    continue\n",
    "                self.batch_history.setdefault(k, []).append(v)\n",
    "        self.step += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        plt.close(self.fig)\n",
    "        self.axes[0].cla()\n",
    "        self.axes[1].cla()\n",
    "\n",
    "        self.axes[0].set_ylim(0, 1.2/self.zoom)\n",
    "        self.axes[1].set_ylim(1-1/self.zoom/2, 1+0.1/self.zoom/2)\n",
    "\n",
    "        self.epoch_step.append(self.step)\n",
    "        for k,v in logs.items():\n",
    "          # only log validation metrics\n",
    "          if not k.startswith('val_'):\n",
    "            continue\n",
    "            self.epoch_history.setdefault(k, []).append(v)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for k,v in self.batch_history.items():\n",
    "            self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.batch_step) / self.steps_per_epoch, v, label=k)\n",
    "\n",
    "        for k,v in self.epoch_history.items():\n",
    "            self.axes[0 if k.endswith('loss') else 1].plot(np.array(self.epoch_step) / self.steps_per_epoch, v, label=k, linewidth=3)\n",
    "\n",
    "        self.axes[0].legend()\n",
    "        self.axes[1].legend()\n",
    "        self.axes[0].set_xlabel('epochs')\n",
    "        self.axes[1].set_xlabel('epochs')\n",
    "        self.axes[0].minorticks_on()\n",
    "        self.axes[0].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
    "        self.axes[0].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
    "        self.axes[1].minorticks_on()\n",
    "        self.axes[1].grid(True, which='major', axis='both', linestyle='-', linewidth=1)\n",
    "        self.axes[1].grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)\n",
    "        display.display(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6927f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def splitToPixels(str):\n",
    "    pixels = []\n",
    "    line = []\n",
    "    curNum = \"\"\n",
    "    for i, c in enumerate(str):\n",
    "        if(c != ' '):\n",
    "            curNum += c\n",
    "        elif(len(curNum) > 0):\n",
    "            line.append(int(curNum))\n",
    "            curNum = \"\"\n",
    "            if(len(line) == 48):\n",
    "                pixels.append(line)\n",
    "                line = []\n",
    "    line.append(int(curNum))\n",
    "    pixels.append(line)      \n",
    "    return pixels\n",
    "\n",
    "def getDataSets(path):\n",
    "    with open(path, 'r') as f:\n",
    "        dataReader = csv.reader(f)\n",
    "        trainLabels = []\n",
    "        trainPixels = []\n",
    "        testLabels = []\n",
    "        testPixels = []\n",
    "        next(dataReader)\n",
    "        for line in dataReader:\n",
    "            if(line[1] == \"Training\" or line[1] == \"PublicTest\"):\n",
    "                trainLabels.append(tf.one_hot(int(line[0]), 7))\n",
    "                trainPixels.append(splitToPixels(line[2]))   \n",
    "            elif(line[1] == \"PrivateTest\"):\n",
    "                testLabels.append(tf.one_hot(int(line[0]), 7))\n",
    "                testPixels.append(splitToPixels(line[2])) \n",
    "            else: print(\"ERROR: Unexpected Data Label\")\n",
    "        \n",
    "        print(len(trainLabels), len(trainPixels), len(testLabels), len(testPixels))\n",
    "        print(\"Reading File Complete\")\n",
    "        \n",
    "        testImg = np.asarray([num for line in trainPixels[len(trainPixels)-1] for num in line]).reshape(IMG_H, IMG_W).astype(\"uint8\")\n",
    "        print(testImg.shape)\n",
    "        plt.imshow(testImg, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        train = tf.data.Dataset.from_tensor_slices((trainPixels, trainLabels))      \n",
    "        test = tf.data.Dataset.from_tensor_slices((testPixels, testLabels))\n",
    "        \n",
    "        return train, test\n",
    "\n",
    "def initDataSets(path, batchSize):\n",
    "    trainDataset, testDataset = getDataSets(path)\n",
    "    \n",
    "    trainDataset = trainDataset.cache()\n",
    "    print(\"Training dataset cached\")\n",
    "    trainDataset = trainDataset.shuffle(5000, reshuffle_each_iteration=True)\n",
    "    print(\"Training dataset shuffled\")\n",
    "    trainDataset = trainDataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    print(\"Training dataset batched\")\n",
    "    trainDataset = trainDataset.prefetch(AUTO)\n",
    "    print(\"Training dataset prefetched\")\n",
    "    \n",
    "    testDataset = testDataset.cache()\n",
    "    print(\"Test dataset cached\")\n",
    "    testDataset = testDataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    print(\"Test dataset batched\")\n",
    "    \n",
    "    return trainDataset, testDataset\n",
    "\n",
    "trainDataset, testDataset = initDataSets(dataPath, BATCH_SIZE)\n",
    "\n",
    "print(\"Completed\")\n",
    "print(trainDataset)\n",
    "\n",
    "# trainInput_fn = lambda: getTrainDataset(trainPath, BATCH_SIZE)\n",
    "# testInput_fn = lambda: getTestDataset(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([element for element in trainDataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da92ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "390cd3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 6, 6, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 3, 3, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 3, 3, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_18 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,379,911\n",
      "Trainable params: 13,369,159\n",
      "Non-trainable params: 10,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "num_classes = 7\n",
    "\n",
    "cnn = tf.keras.models.Sequential([\n",
    "    #1st\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(48,48,1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #2nd\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #3rd\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #4th\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #5th\n",
    "    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    #Dense\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff8e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 48, 48, 1)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 10, 10, 128)       15616     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 10, 10, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 5, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 5, 5, 256)         819456    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 5, 5, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 2, 2, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 2, 2, 256)         65792     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 2, 256)         65792     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              263168    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,881,287\n",
      "Trainable params: 2,878,983\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape(input_shape=(48, 48), target_shape=(48, 48, 1)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(11, 11), strides=(4, 4), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(7, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.001),\n",
    "    metrics=['accuracy']    \n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_training = PlotTraining(sample_rate=10, zoom=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda23750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr decay function\n",
    "def lr_decay(epoch):\n",
    "    return 0.01 * math.pow(0.666, epoch)\n",
    "\n",
    "# lr schedule callback\n",
    "lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
    "\n",
    "# important to see what you are doing\n",
    "plot_learning_rate(lr_decay, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98364d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = NUM_DATA//BATCH_SIZE\n",
    "print(\"Steps per epoch: \", steps_per_epoch)\n",
    "\n",
    "history = model.fit(trainDataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                    validation_data=testDataset, validation_steps=1, callbacks=[plot_training, lr_decay_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = //BATCH_SIZE\n",
    "\n",
    "history = model.fit(trainDataset, validation_data=testDataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, callbacks=[plot_training, lr_decay_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a1641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "487/487 [==============================] - 168s 340ms/step - loss: 2.1573 - accuracy: 0.2032 - val_loss: 1.7834 - val_accuracy: 0.2612\n",
      "Epoch 2/50\n",
      "487/487 [==============================] - 163s 334ms/step - loss: 1.9769 - accuracy: 0.2261 - val_loss: 1.7708 - val_accuracy: 0.2779\n",
      "Epoch 3/50\n",
      "487/487 [==============================] - 159s 327ms/step - loss: 1.8959 - accuracy: 0.2449 - val_loss: 1.7423 - val_accuracy: 0.3016\n",
      "Epoch 4/50\n",
      "487/487 [==============================] - 159s 326ms/step - loss: 1.8440 - accuracy: 0.2577 - val_loss: 1.7233 - val_accuracy: 0.3103\n",
      "Epoch 5/50\n",
      "487/487 [==============================] - 160s 329ms/step - loss: 1.8008 - accuracy: 0.2775 - val_loss: 1.7105 - val_accuracy: 0.3225\n",
      "Epoch 6/50\n",
      "487/487 [==============================] - 151s 310ms/step - loss: 1.7662 - accuracy: 0.2954 - val_loss: 1.6908 - val_accuracy: 0.3329\n",
      "Epoch 7/50\n",
      "487/487 [==============================] - 105s 216ms/step - loss: 1.7322 - accuracy: 0.3058 - val_loss: 1.6579 - val_accuracy: 0.3530\n",
      "Epoch 8/50\n",
      "487/487 [==============================] - 118s 242ms/step - loss: 1.6958 - accuracy: 0.3301 - val_loss: 1.6413 - val_accuracy: 0.3518\n",
      "Epoch 9/50\n",
      "487/487 [==============================] - 127s 260ms/step - loss: 1.6695 - accuracy: 0.3426 - val_loss: 1.6208 - val_accuracy: 0.3552\n",
      "Epoch 10/50\n",
      "487/487 [==============================] - 127s 261ms/step - loss: 1.6336 - accuracy: 0.3591 - val_loss: 1.5935 - val_accuracy: 0.3772\n",
      "Epoch 11/50\n",
      "487/487 [==============================] - 126s 258ms/step - loss: 1.6085 - accuracy: 0.3708 - val_loss: 1.5790 - val_accuracy: 0.3884\n",
      "Epoch 12/50\n",
      "487/487 [==============================] - 126s 260ms/step - loss: 1.5835 - accuracy: 0.3822 - val_loss: 1.5962 - val_accuracy: 0.3800\n",
      "Epoch 13/50\n",
      "487/487 [==============================] - 131s 269ms/step - loss: 1.5607 - accuracy: 0.3915 - val_loss: 1.5388 - val_accuracy: 0.4065\n",
      "Epoch 14/50\n",
      "487/487 [==============================] - 127s 260ms/step - loss: 1.5329 - accuracy: 0.4039 - val_loss: 1.5573 - val_accuracy: 0.3895\n",
      "Epoch 15/50\n",
      "487/487 [==============================] - 123s 253ms/step - loss: 1.5074 - accuracy: 0.4161 - val_loss: 1.5858 - val_accuracy: 0.3836\n",
      "Epoch 16/50\n",
      "487/487 [==============================] - 125s 256ms/step - loss: 1.4838 - accuracy: 0.4258 - val_loss: 1.5375 - val_accuracy: 0.4093\n",
      "Epoch 17/50\n",
      "487/487 [==============================] - 125s 256ms/step - loss: 1.4625 - accuracy: 0.4361 - val_loss: 1.5120 - val_accuracy: 0.4227\n",
      "Epoch 18/50\n",
      "487/487 [==============================] - 135s 277ms/step - loss: 1.4337 - accuracy: 0.4469 - val_loss: 1.5806 - val_accuracy: 0.3839\n",
      "Epoch 19/50\n",
      "487/487 [==============================] - 128s 263ms/step - loss: 1.4168 - accuracy: 0.4537 - val_loss: 1.4883 - val_accuracy: 0.4249\n",
      "Epoch 20/50\n",
      "487/487 [==============================] - 122s 251ms/step - loss: 1.3898 - accuracy: 0.4654 - val_loss: 1.6057 - val_accuracy: 0.4113\n",
      "Epoch 21/50\n",
      "487/487 [==============================] - 124s 254ms/step - loss: 1.3662 - accuracy: 0.4756 - val_loss: 1.4937 - val_accuracy: 0.4244\n",
      "Epoch 22/50\n",
      "487/487 [==============================] - 122s 250ms/step - loss: 1.3438 - accuracy: 0.4829 - val_loss: 1.5590 - val_accuracy: 0.3915\n",
      "Epoch 23/50\n",
      "487/487 [==============================] - 125s 256ms/step - loss: 1.3164 - accuracy: 0.4963 - val_loss: 1.5937 - val_accuracy: 0.4149\n",
      "Epoch 24/50\n",
      "487/487 [==============================] - 124s 255ms/step - loss: 1.2937 - accuracy: 0.5074 - val_loss: 1.6498 - val_accuracy: 0.3664\n",
      "Epoch 25/50\n",
      "487/487 [==============================] - 125s 256ms/step - loss: 1.2658 - accuracy: 0.5159 - val_loss: 2.3677 - val_accuracy: 0.2843\n",
      "Epoch 26/50\n",
      "487/487 [==============================] - 124s 255ms/step - loss: 1.2386 - accuracy: 0.5289 - val_loss: 1.6329 - val_accuracy: 0.4302\n",
      "Epoch 27/50\n",
      "487/487 [==============================] - 124s 255ms/step - loss: 1.2173 - accuracy: 0.5358 - val_loss: 1.6319 - val_accuracy: 0.4406\n",
      "Epoch 28/50\n",
      "487/487 [==============================] - 129s 266ms/step - loss: 1.1883 - accuracy: 0.5481 - val_loss: 1.5505 - val_accuracy: 0.4311\n",
      "Epoch 29/50\n",
      "487/487 [==============================] - 128s 263ms/step - loss: 1.1696 - accuracy: 0.5570 - val_loss: 1.5320 - val_accuracy: 0.4350\n",
      "Epoch 30/50\n",
      "487/487 [==============================] - 127s 262ms/step - loss: 1.1382 - accuracy: 0.5690 - val_loss: 1.5969 - val_accuracy: 0.4216\n",
      "Epoch 31/50\n",
      "487/487 [==============================] - 126s 259ms/step - loss: 1.1094 - accuracy: 0.5799 - val_loss: 1.4409 - val_accuracy: 0.4715\n",
      "Epoch 32/50\n",
      "487/487 [==============================] - 125s 257ms/step - loss: 1.0836 - accuracy: 0.5879 - val_loss: 2.4825 - val_accuracy: 0.3153\n",
      "Epoch 33/50\n",
      "487/487 [==============================] - 125s 257ms/step - loss: 1.0612 - accuracy: 0.5976 - val_loss: 1.5456 - val_accuracy: 0.4531\n",
      "Epoch 34/50\n",
      "487/487 [==============================] - 121s 248ms/step - loss: 1.0369 - accuracy: 0.6119 - val_loss: 1.6151 - val_accuracy: 0.4467\n",
      "Epoch 35/50\n",
      "487/487 [==============================] - 109s 224ms/step - loss: 1.0151 - accuracy: 0.6196 - val_loss: 1.6102 - val_accuracy: 0.4355\n",
      "Epoch 36/50\n",
      "487/487 [==============================] - 108s 223ms/step - loss: 0.9827 - accuracy: 0.6344 - val_loss: 1.8990 - val_accuracy: 0.4071\n",
      "Epoch 37/50\n",
      "487/487 [==============================] - 110s 226ms/step - loss: 0.9528 - accuracy: 0.6436 - val_loss: 1.8279 - val_accuracy: 0.4199\n",
      "Epoch 38/50\n",
      "487/487 [==============================] - 108s 222ms/step - loss: 0.9254 - accuracy: 0.6529 - val_loss: 1.8504 - val_accuracy: 0.3998\n",
      "Epoch 39/50\n",
      "487/487 [==============================] - 110s 227ms/step - loss: 0.8956 - accuracy: 0.6693 - val_loss: 1.7757 - val_accuracy: 0.4127\n",
      "Epoch 40/50\n",
      "487/487 [==============================] - 109s 224ms/step - loss: 0.8671 - accuracy: 0.6796 - val_loss: 2.1639 - val_accuracy: 0.3675\n",
      "Epoch 41/50\n",
      "487/487 [==============================] - 109s 224ms/step - loss: 0.8376 - accuracy: 0.6892 - val_loss: 2.1666 - val_accuracy: 0.4177\n",
      "Epoch 42/50\n",
      "487/487 [==============================] - 109s 224ms/step - loss: 0.8111 - accuracy: 0.7015 - val_loss: 1.7369 - val_accuracy: 0.4263\n",
      "Epoch 43/50\n",
      "487/487 [==============================] - 110s 226ms/step - loss: 0.7896 - accuracy: 0.7122 - val_loss: 1.7550 - val_accuracy: 0.4436\n",
      "Epoch 44/50\n",
      "487/487 [==============================] - 141s 289ms/step - loss: 0.7579 - accuracy: 0.7243 - val_loss: 1.7317 - val_accuracy: 0.4590\n",
      "Epoch 45/50\n",
      "487/487 [==============================] - 151s 310ms/step - loss: 0.7325 - accuracy: 0.7336 - val_loss: 1.7699 - val_accuracy: 0.4489\n",
      "Epoch 46/50\n",
      "487/487 [==============================] - 143s 293ms/step - loss: 0.7083 - accuracy: 0.7429 - val_loss: 2.0845 - val_accuracy: 0.4403\n",
      "Epoch 47/50\n",
      "487/487 [==============================] - 148s 305ms/step - loss: 0.6836 - accuracy: 0.7488 - val_loss: 1.8375 - val_accuracy: 0.4648\n",
      "Epoch 48/50\n",
      "487/487 [==============================] - 136s 280ms/step - loss: 0.6487 - accuracy: 0.7669 - val_loss: 1.9430 - val_accuracy: 0.4411\n",
      "Epoch 49/50\n",
      "487/487 [==============================] - 140s 286ms/step - loss: 0.6255 - accuracy: 0.7772 - val_loss: 1.8775 - val_accuracy: 0.4498\n",
      "Epoch 50/50\n",
      "487/487 [==============================] - 140s 287ms/step - loss: 0.5899 - accuracy: 0.7888 - val_loss: 2.1755 - val_accuracy: 0.4634\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    trainDataset,\n",
    "    epochs=50,\n",
    "    validation_data=testDataset,\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05931058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34820, 3)\n",
      "31231\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6369db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34820, 3)\n",
      "31231\n",
      "(31231, 48, 48, 1)\n",
      "(31231, 7)\n",
      "(3589, 48, 48, 1)\n",
      "X_train Shape: (15615, 48, 48, 1)\n",
      "y_train Shape: (15615, 7)\n",
      "\n",
      "X_valid Shape: (15616, 48, 48, 1)\n",
      "y_valid Shape: (15616, 7)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(dataPath)\n",
    "data.columns = ['emotion', 'Usage', 'pixels']\n",
    "print(data.shape)\n",
    "\n",
    "testDS = data.loc[data[\"Usage\"] == 'PrivateTest',['emotion','pixels']]\n",
    "trainDS = data.loc[data[\"Usage\"] != 'PrivateTest',['emotion','pixels']]\n",
    "\n",
    "print(len(trainDS))\n",
    "\n",
    "trainDS['pixels'] = [np.fromstring(line, dtype=int, sep=' ').reshape(-1,48,48,1) for line in trainDS['pixels']]\n",
    "testDS['pixels'] = [np.fromstring(line, dtype=int, sep=' ').reshape(-1,48,48,1) for line in testDS['pixels']]\n",
    "\n",
    "trainPixels = np.concatenate(trainDS['pixels'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(trainDS.emotion)\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "print(trainPixels.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "testPixels = np.concatenate(testDS['pixels'].values)\n",
    "\n",
    "print(testPixels.shape)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    trainPixels, labels, test_size=0.5, stratify=labels, random_state=1\n",
    ")\n",
    "\n",
    "\n",
    "print('X_train Shape:', X_train.shape)\n",
    "print('y_train Shape:', y_train.shape)\n",
    "print()\n",
    "print('X_valid Shape:', X_valid.shape)\n",
    "print('y_valid Shape:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f04223",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_flow = train_datagen.flow(X_train, y_train, batch_size=64)\n",
    "val_flow = val_datagen.flow(X_valid, y_valid, batch_size=64)\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.00005,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663bacec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6cdb8e8f-b181-498b-ad45-698978670c58/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6cdb8e8f-b181-498b-ad45-698978670c58/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('AlexNet.h5')\n",
    "pickle.dump(history, open(f'AlexNet.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "243/243 [==============================] - 7541s 31s/step - loss: 2.2388 - accuracy: 0.1909 - val_loss: 1.9821 - val_accuracy: 0.2519 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      " 42/243 [====>.........................] - ETA: 12:55 - loss: 2.0422 - accuracy: 0.2083"
     ]
    }
   ],
   "source": [
    "epochs = 100 \n",
    "batch_size = 64\n",
    "h1 = cnn.fit(\n",
    "    train_flow,\n",
    "    steps_per_epoch=len(X_train) / batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=val_flow,\n",
    "    validation_steps=len(X_valid) / batch_size,\n",
    "    callbacks=callbacks,\n",
    "    workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcc09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
